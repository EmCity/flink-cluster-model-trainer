{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import src.misc.evaluation as evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read data\n",
    "#training_files = \"../../dataset/training/\"\n",
    "#trajectories_file = \"trajectories(table 5)_training.csv\"\n",
    "#volume_file = \"volume(table 6)_training.csv\"\n",
    "#trajectories_df = pd.read_csv(training_files+trajectories_file)\n",
    "#volume_df = pd.read_csv(training_files+volume_file)\n",
    "\n",
    "#import src.vector_gen.generateCurrentSituationWithTime as gcswt\n",
    "#import src.vector_gen.generate_VectorY as gvy\n",
    "#y_df = gvy.generate_VectorY_df(trajectories_df)\n",
    "\n",
    "\n",
    "# splited\n",
    "training_Y = pd.read_csv(\"src/misc/train_Y.csv\", index_col =0)\n",
    "testing_Y = pd.read_csv(\"src/misc/test_Y.csv\", index_col =0)\n",
    "training_X = pd.read_csv(\"src/misc/train_X.csv\", index_col =0)\n",
    "testing_X = pd.read_csv(\"src/misc/test_X.csv\", index_col =0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1020 48\n",
      "(1020, 147) (1020, 36)\n",
      "x:  147\n",
      "y:  36\n"
     ]
    }
   ],
   "source": [
    "print(len(training_X), len(testing_Y))\n",
    "print(training_X.shape, training_Y.shape)\n",
    "x_dim = len(training_X.columns)\n",
    "y_dim = len(training_Y.columns)\n",
    "print('x: ', x_dim)\n",
    "print('y: ', y_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://radiostud.io/beat-rush-hour-traffic-with-tensorflow-machine-learning/\n",
    "# https://www.youtube.com/watch?v=PwAGxqrXSCs\n",
    "# https://www.tensorflow.org/get_started/mnist/beginners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model\n",
    "x = tf.placeholder(tf.float32, [None, x_dim], name=\"x\")\n",
    "y = tf.placeholder(tf.float32, [None, y_dim], name=\"y\")\n",
    "\n",
    "\n",
    "# one layer\n",
    "# y_pred = x * weight + bias\n",
    "weights = tf.Variable(tf.ones([x_dim, y_dim], dtype=tf.float32), name=\"weight\")\n",
    "biases = tf.Variable(tf.zeros([y_dim], dtype=tf.float32), name=\"bias\")\n",
    "\n",
    "y_pred = tf.add(tf.matmul(x, weights), biases)\n",
    "\n",
    "# activation function, relu rectified linear\n",
    "# https://www.tensorflow.org/api_guides/python/nn\n",
    "#y_pred = tf.nn.relu(y_pred)\n",
    "\n",
    "\n",
    "# cost\n",
    "with tf.name_scope(\"cost_func\"):\n",
    "    # def cost/loss function\n",
    "    #cost_func = tf.reduce_mean(evaluation.mape2(y_pred=y_pred, y_true=y))\n",
    "    #cost_func = tf.metrics.mean_absolute_error(y_pred, y)\n",
    "    #cost_func = tf.reduce_mean(tf.metrics.mean_absolute_error(y_pred, y))\n",
    "    #cost_func = -tf.reduce_sum(y*tf.log(y_pred))\n",
    "    cost_func = tf.reduce_mean(tf.div(tf.abs(y_pred-y), y))\n",
    "\n",
    "#train\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(cost_func)\n",
    "    #optimizer = tf.train.AdamOptimizer().minimize(cost_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 23323, training accuracy 27.1096\n",
      "epoch 1, loss 7615, training accuracy 4.50463\n",
      "epoch 2, loss 1828, training accuracy 0.74091\n",
      "epoch 3, loss 1051, training accuracy 0.482714\n",
      "epoch 4, loss 931, training accuracy 0.419372\n",
      "epoch 5, loss 844, training accuracy 0.431106\n",
      "epoch 6, loss 783, training accuracy 0.384041\n",
      "epoch 7, loss 738, training accuracy 0.33008\n",
      "epoch 8, loss 702, training accuracy 0.364702\n",
      "epoch 9, loss 671, training accuracy 0.330244\n",
      "epoch 10, loss 644, training accuracy 0.288913\n",
      "epoch 11, loss 622, training accuracy 0.303864\n",
      "epoch 12, loss 602, training accuracy 0.290959\n",
      "epoch 13, loss 584, training accuracy 0.280861\n",
      "epoch 14, loss 568, training accuracy 0.27843\n",
      "epoch 15, loss 553, training accuracy 0.26571\n",
      "epoch 16, loss 540, training accuracy 0.265181\n",
      "epoch 17, loss 528, training accuracy 0.248774\n",
      "epoch 18, loss 516, training accuracy 0.243498\n",
      "epoch 19, loss 506, training accuracy 0.271226\n",
      "epoch 20, loss 497, training accuracy 0.27626\n",
      "epoch 21, loss 488, training accuracy 0.252426\n",
      "epoch 22, loss 480, training accuracy 0.263913\n",
      "epoch 23, loss 472, training accuracy 0.248076\n",
      "epoch 24, loss 465, training accuracy 0.244745\n",
      "epoch 25, loss 459, training accuracy 0.256228\n",
      "epoch 26, loss 452, training accuracy 0.266013\n",
      "epoch 27, loss 447, training accuracy 0.24262\n",
      "epoch 28, loss 442, training accuracy 0.246797\n",
      "epoch 29, loss 437, training accuracy 0.244983\n",
      "Epoch 29 loss 437.313999414\n",
      "mean MAPE\n",
      " 0.5673510600607816\n",
      "MAPE\n",
      " (0, 'A2')    0.310303\n",
      "(0, 'A3')    0.259794\n",
      "(0, 'B1')    0.763500\n",
      "(0, 'B3')    0.328908\n",
      "(0, 'C1')    1.350998\n",
      "(0, 'C3')    0.584215\n",
      "(1, 'A2')    0.322280\n",
      "(1, 'A3')    0.317750\n",
      "(1, 'B1')    0.633285\n",
      "(1, 'B3')    0.384761\n",
      "(1, 'C1')    0.676494\n",
      "(1, 'C3')    1.141455\n",
      "(2, 'A2')    0.255639\n",
      "(2, 'A3')    0.340932\n",
      "(2, 'B1')    0.680145\n",
      "(2, 'B3')    0.367110\n",
      "(2, 'C1')    0.452015\n",
      "(2, 'C3')    1.213254\n",
      "(3, 'A2')    0.318896\n",
      "(3, 'A3')    0.260254\n",
      "(3, 'B1')    0.676212\n",
      "(3, 'B3')    0.321978\n",
      "(3, 'C1')    0.665480\n",
      "(3, 'C3')    0.702309\n",
      "(4, 'A2')    0.343952\n",
      "(4, 'A3')    0.368121\n",
      "(4, 'B1')    0.646947\n",
      "(4, 'B3')    0.386585\n",
      "(4, 'C1')    0.833131\n",
      "(4, 'C3')    0.691485\n",
      "(5, 'A2')    0.324430\n",
      "(5, 'A3')    0.309331\n",
      "(5, 'B1')    0.695929\n",
      "(5, 'B3')    0.412945\n",
      "(5, 'C1')    0.972528\n",
      "(5, 'C3')    1.111288\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# start session and train\n",
    "epochs = 30\n",
    "batch_size = 1\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "tf.summary.scalar('cost', cost_func)\n",
    "tf.summary.histogram('weights', weights)\n",
    "tf.summary.histogram('biases', biases)\n",
    "merged_summary = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter('./tftrain', sess.graph)\n",
    "#test_writer = tf.summary.FileWriter('./tftest')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for batch in range(0, int(len(training_X)/batch_size)):\n",
    "        x_batch = training_X[batch*batch_size: batch*batch_size+batch_size]\n",
    "        y_batch = training_Y[batch*batch_size: batch*batch_size+batch_size]\n",
    "        \n",
    "        # Occasionally report accuracy\n",
    "        #if batch % 100 == 0:\n",
    "        #    [train_accuracy] = sess.run([cost_func], feed_dict={x: x_batch, y: y_batch})\n",
    "        #    print(\"epoch %d, batchstep %d, training accuracy %g\" % (epoch, batch, train_accuracy))\n",
    "            \n",
    "\n",
    "        #run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "        #run_metadata = tf.RunMetadata()\n",
    "        #\n",
    "        \n",
    "        # train\n",
    "        _, c = sess.run([optimizer, cost_func], feed_dict={x: x_batch, y: y_batch})\n",
    "        \n",
    "        epoch_loss += c\n",
    "        \n",
    "    [train_accuracy] = sess.run([cost_func], feed_dict={x: x_batch, y: y_batch})\n",
    "    print(\"epoch %d, loss %d, training accuracy %g\" % (epoch, epoch_loss, train_accuracy))\n",
    "    s = sess.run(merged_summary, feed_dict={x: x_batch, y: y_batch})\n",
    "    writer.add_summary(s, epoch)\n",
    "\n",
    "print('Epoch', epoch, 'loss', epoch_loss)\n",
    "\n",
    "# TODO FALSCH!!!???\n",
    "prediction = y_pred.eval(feed_dict={x: testing_X}, session = sess)\n",
    "mape = evaluation.mape(prediction, testing_Y)\n",
    "\n",
    "print('mean MAPE\\n', np.mean(mape))\n",
    "\n",
    "print('MAPE\\n', mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pd.DataFrame(prediction, index=testing_Y.index, columns=testing_Y.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#testing_Y"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}